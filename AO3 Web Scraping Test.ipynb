{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50804632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55f8caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory if it does not exist\n",
    "if not os.path.exists('fanfics'):\n",
    "    os.mkdir('fanfics')\n",
    "\n",
    "# Loop through the 100 pages of fanfiction and save HTML files to \"fanfics\" folder\n",
    "for x in range(1, 5000, 20):\n",
    "    url = f'https://archiveofourown.org/tags/%E5%83%95%E3%81%AE%E3%83%92%E3%83%BC%E3%83%AD%E3%83%BC%E3%82%A2%E3%82%AB%E3%83%87%E3%83%9F%E3%82%A2%20|%20Boku%20no%20Hero%20Academia%20|%20My%20Hero%20Academia%20(Anime%20*a*%20Manga)/works?page={x}'\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            html = response.read()\n",
    "            html = html.decode('utf-8')\n",
    "\n",
    "        with open (f'fanfics/AO3_Page{x}_spread.html', 'w', encoding=\"utf-8\") as new_file:\n",
    "            new_file.write(html)\n",
    "        time.sleep(8)\n",
    "    except urllib.error.HTTPError as e:\n",
    "        if e.code == 429:\n",
    "                print('Too many requests!---SLEEPING---')\n",
    "                print('we should restart on page', x)\n",
    "                print('we should restart with this url:', url)\n",
    "                break\n",
    "        raise\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e439815",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating soup objects and putting them in a list\n",
    "AO3_list = []\n",
    "for i in range(1, 5000, 20):\n",
    "    html = f'fanfics/AO3_Page{i}_spread.html'\n",
    "    ao3_soup = BeautifulSoup(open(html,encoding='utf-8'), 'html.parser')\n",
    "    AO3_list.append(ao3_soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d931053",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://archiveofourown.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72c069fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWebsiteAsSoup(url):\n",
    "\n",
    "    req = urllib.request.Request(url)\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        story_html = response.read()\n",
    "    \n",
    "    story_soup = BeautifulSoup(story_html, 'html.parser')\n",
    "    with open('story_spread.html', 'w') as new_file:\n",
    "        new_file.write(str(story_soup))\n",
    "        \n",
    "    return story_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0688c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = []\n",
    "fanfics = []\n",
    "relationship = []\n",
    "character = []\n",
    "mix_fandom = []\n",
    "\n",
    "for x in range(len(AO3_list)):\n",
    "    for titles in AO3_list[x].find_all('li', role = 'article'):\n",
    "        fanfic = {}\n",
    "        title = titles.find('a').get_text()\n",
    "        fanfic['title'] = title\n",
    "        \n",
    "        url_tail = titles.find('a').get('href')\n",
    "        link = url + url_tail\n",
    "        stories.append([title, link])\n",
    "        \n",
    "        fandom = titles.find_all('h5', class_ = 'fandoms heading')\n",
    "        fandoms = [tag.get_text().strip() for tag in fandom]\n",
    "        fanfic['mixed fandom'] = fandoms\n",
    "        mix_fandom.append(fandoms)\n",
    "        \n",
    "        relationships = titles.find_all('li', class_ = 'relationships')\n",
    "        ships = [tag.get_text().strip() for tag in relationships]\n",
    "        fanfic['ships'] = ships\n",
    "        \n",
    "        char = titles.find_all('li', class_ = 'characters')\n",
    "        characters = [tag.get_text() for tag in char]\n",
    "        fanfic['characters'] = (characters)\n",
    "        \n",
    "        try:\n",
    "            hits = titles.find('dd', 'hits').get_text()\n",
    "        except:\n",
    "            hits = None\n",
    "        fanfic['hits'] = hits\n",
    "        \n",
    "        try:\n",
    "            kudos = titles.find('dd', 'kudos').get_text()\n",
    "        except:\n",
    "            kudos = None\n",
    "        fanfic['kudos'] = kudos\n",
    "        \n",
    "        try:\n",
    "            fic_soup = getWebsiteAsSoup(link)\n",
    "            fic_soup.prettify()\n",
    "            for info in fic_soup:\n",
    "                published = info.find('dd', class_ = 'published').get_text()\n",
    "                fanfic['published'] = published\n",
    "                relationship.append([ships, published])\n",
    "                character.append([characters, published])\n",
    "        except:\n",
    "            published = titles.find('p', class_ = 'datetime').get_text()\n",
    "            fanfic['published'] = published\n",
    "            character.append([characters, published])\n",
    "            relationship.append([ships, published])\n",
    "        fanfics.append(fanfic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "118edec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21 Apr 2023    20\n",
       "31 May 2022    20\n",
       "27 Jul 2022    20\n",
       "19 Jul 2022    20\n",
       "20 Apr 2023    20\n",
       "               ..\n",
       "13 Feb 2022     1\n",
       "07 Feb 2022     1\n",
       "08 Dec 2022     1\n",
       "18 Mar 2023     1\n",
       "13 Dec 2022     1\n",
       "Name: published, Length: 328, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fanfics_df = pd.DataFrame.from_dict(fanfics, orient = 'columns')\n",
    "fanfics_df['published'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b37dcc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fanfics_df.to_csv(\"AO3_spread.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d868c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "shipings = pd.DataFrame(relationship)\n",
    "shipings.columns = ['ships', 'dates']\n",
    "ships = shipings[219:]\n",
    "ships = ships.explode('ships')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "637c4c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95fa7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "ships.to_csv(\"shipsviadate_spread.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85f00054",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = pd.DataFrame(character)\n",
    "characters.columns = ['names', 'dates']\n",
    "characters = characters[65:]\n",
    "characters = characters.explode('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af24ebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "character.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81db1506",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters.to_csv('charactersviadate_spread.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bcee12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
